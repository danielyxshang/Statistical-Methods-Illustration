---
title: "Statistical Methods Illustration - Classification Methods"
author: "Daniel Shang"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load the necessary packages
library(tidylog)
library(e1071)
library(ggplot2)
library(party)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(ggthemes)
library(car)
library(pROC)
```

```{r}
# Load the data, remove missing values (if any), and convert columns to proper formats
## based on the documentation of the dataset
data = read.csv('heart.csv', )
data_clean = na.omit(mutate_all(data, 
                      ~ifelse(. %in% c("N/A", "null", "", NULL),  NA, .)))
colnames(data_clean)[1] = 'age'
data_clean$sex = as.factor(data_clean$sex)
data_clean$cp = as.factor(data_clean$cp)
data_clean$fbs = as.factor(data_clean$fbs)
data_clean$restecg = as.factor(data_clean$restecg)
data_clean$exang = as.factor(data_clean$exang)
data_clean$slope = as.factor(data_clean$slope)
data_clean$ca = as.factor(data_clean$ca)
data_clean$thal = as.factor(data_clean$thal)
data_clean$target = as.factor(data_clean$target)
```

```{r}
# Set up a train/test split for later model evaluation
set.seed(111)
index_train_test = sample(x = 2, size = nrow(data_clean), replace = TRUE, prob = c(0.8, 0.2))
train_data = data_clean[index_train_test == 1, ]
test_data = data_clean[index_train_test == 2, ]
```

------------------------------------------- Support Vector Machine -------------------------------------------
```{r}
# Build a support vector machine (SVM) to predict the 'target' and summarize the model
svm_model = svm(target ~ ., data = train_data, kernel = 'linear')
summary(svm_model)
```

```{r}
# Make predictions using the model and compare the outcome with the 'target' values
## in the dataset. Summarize the prediction accuracy and related statistics using
## a confusion matrix
set.seed(123)
prediction_svm1 = predict(svm_model, newdata = test_data)
confusionMatrix(prediction_svm1, test_data$target)
```

```{r}
# Use the 'tune' formula to figure out the best parameters for the SVM model to
## boost the model performance. The darker the color is, the better the model will
## perform, as indicated by the 'cost' y-axis label.
set.seed(123)
tune_svm = tune(svm, target ~ ., data = train_data, range = list(epsilon = seq(0, 2, 0.1), cost = 2^(2:5)))
plot(tune_svm)
```

```{r}
# Summarize the tuned model
summary(tune_svm)
```

```{r}
# Use the best model tuned by the function and set it as our final model
set.seed(123)
final_svm = tune_svm$best.model
summary(final_svm)
```

```{r}
# Use the tuned model to make prediction and compare the accuracy with the previous
## model. Since the prediction accuracy increased from 0.8779 to 0.9241, we can
## conclude that the 'tune' function did a great job identifying the best model
prediction_svm2 = predict(final_svm, newdata = test_data)
confusionMatrix(prediction_svm2, test_data$target)
```

--------------------------------------------- Classification Tree --------------------------------------------
```{r}
# Build a classification tree model to predict the target. I used a tree control
## parameter 'mincriterion.' The value of this parameter will be considered as 
## 1 - p-value that must be exceeded in order to implement a node split.
tree_model1 = ctree(target~., data = train_data, controls = ctree_control(mincriterion = 0.95))
summary(tree_model1)
tree_model1
```

```{r}
# Plot the tree model built
plot(tree_model1, type = 'simple')
```

```{r}
# Make prediction using the tree model and build a confusion matrix to evaluate
## its prediction accuracy
prediction_tree1 = predict(tree_model1, newdata = test_data)
confusionMatrix(prediction_tree1, test_data$target)
```

```{r}
# Build another tree model using a different package
tree_model2 = rpart(target ~ ., data = train_data)
```

```{r}
# Plot the tree at a certain level of detail
rpart.plot(tree_model2, extra = 1)
```

```{r}
# Make prediction using the second tree model and build a confusion matrix to evaluate
## the prediction accuracy
prediction_tree2 = predict(tree_model2, newdata = test_data, type = 'class')
confusionMatrix(prediction_tree2, test_data$target)
```

----------------------------------------------- Random Forest ----------------------------------------------
```{r}
# Build a random forest model to predict the 'target' variable in the dataset. I
## started with a huge number of trees (ntree) so that, based on the plot later,
## we can easily identify the number of trees that leads to least prediction error
set.seed(123)
rf_model1 = randomForest(target ~ ., data = train_data, ntree = 2000)
print(rf_model1)
```

```{r}
# Use the random forest model to make prediction and build a confusion matrix to
## evaluate the prediction accuracy
prediction_rf1 = predict(rf_model1, newdata = test_data)
confusionMatrix(prediction_rf1, test_data$target)
```

```{r}
# Plot the relationship between the number of trees and the prediction error. We
## can see that the error reaches the lowest point when the number of trees is
## around 750. Therefore, I will use this number to build a new model later to see
## if it does a great job predicting
plot(rf_model1)
```

```{r}
# Use the 'tuneRF' function to figure out the 'mtry' parameter that leads to least
## prediction error. 'mtry' is the number of variables randomly sampled as candidates
## at each split of node. According to the plot, an 'mtry' of three leads to the
## random forest model the predicts most accurately
set.seed(123)
tune_rf1 = tuneRF(train_data[, -14], train_data[, 14], stepFactor = 1.5, plot = TRUE, ntreeTry = 750, trace = TRUE, improve = 0.01)
```

```{r}
# Build a new model using the parameters we just figured out. We can see that the
## Out Of Bag (OBB) estimate of error rate decreases from 16.17% to 15.84%, meaning
## that the functions did a great job identifying the best parameters
set.seed(123)
rf_model2 = randomForest(target ~ ., data = train_data, ntree = 750, mtry = 3, importance = TRUE, proximity = TRUE)
print(rf_model2)
```

```{r}
# Build a confusion matrix for more detailed statistics about the model performance
prediction_rf2 = predict(rf_model2, newdata = test_data)
confusionMatrix(prediction_rf2, test_data$target)
```

```{r}
# Plot the distribution of tree size to better understand the model
hist(treesize(rf_model2), col = 'chartreuse1')
```

```{r}
# Plot all the variables in the dataset and sort them based on their relative
## importance when making the prediction. The first plot gives information about
## how much prediction accuracy will decrease if we remove the variable. For example,
## if we remove 'ca,' the prediction accuracy will decrease by 30%. The second plot
## shows how pure the nodes are at the end of the tree, if the variable is removed.
varImpPlot(rf_model2, main = 'Variable importance (high to low)')
```

```{r}
# To know how many times each column is used in the entire random forest, we can
## use the 'varUsed' function
varUsed(rf_model2)
```

```{r}
# To understand the marginal effect of a variable on the final prediction result, 
## we can use the partial dependence plot. For example, this plot shows that, when 
## age is greater than 53, the random forest is much less likely to predict
## 1 as the target for that record.
partialPlot(x = rf_model2, pred.data = test_data, x.var = age, which.class = '1',
            ylab = 'Marginal importance of age')
```

------------------------------------------ K-Nearest Neighbor (KNN) ------------------------------------------
```{r}
# Build a KNN model to predict the 'target'. I started by defining the training
## controls. Here, I will evaluate the KNN model using repeated 10-fold cross
## validation repeated for three times.
trCrl1 = trainControl(method = 'repeatedcv', number = 10, repeats = 5)
```

```{r}
# Then, we train the model using the KNN method. Since different fields in the data
## may have different unit, we pre-processed the data by first minus the mean value
## from each single value. Then, we divide the result by the standard deviation.
## This standardization should result in data that falls within a scope from -3 to 3.
## Only numeric values, not factors, are standardized. The tuneLength means the
## number of K we want to test. To figure out the best number of K, we set it to
## a fairly large number.
set.seed(123)
knn1 = train(data = train_data, target ~ ., method = 'knn', tuneLength = 100,
             trControl = trCrl1, preProcess = c('center', 'scale'))
```

```{r}
# Here, we summarize the model. The result shows that, when k equals 57, the model
## has the highest prediction accuracy. Therefore, if we were to judge the model
## by its prediction accuracy, we get the best model when k equals to 57.
knn1
```

```{r}
# We plot the relationship between k and the model's prediction accuracy. We can
## see that, when k equals to 57, the model performs the best.
plot(knn1)
```

```{r}
# To better understand the model in terms of which field plays the most important role,
## we use the varImp function to sort the importance of different fields in a
## descending order.
varImp(knn1)
```

```{r}
# We predict the target using the model and build a confusion matrix. The result
## shows a prediction accuracy of 84.62%.
confusionMatrix(predict(knn1, newdata = test_data), test_data$target)
```

```{r}
# Other than accuracy, ROC is another common way to evaluate the predictive performance
## of models. I will use ROC to evaluate our KNN model to see if a different K is
## chosen.

set.seed(123)
train_data1 = train_data
test_data1 = test_data
train_data1$target = as.integer(train_data1$target)
test_data1$target = as.integer(test_data1$target)
train_data1$target[train_data1$target == 1] = 'No'
train_data1$target[train_data1$target == 2] = 'Yes'
test_data1$target[test_data1$target == 1] = 'No'
test_data1$target[test_data1$target == 2] = 'Yes'

trCrl2 = trainControl(method = 'repeatedcv', number = 10, repeats = 5,
                      classProbs = TRUE, summaryFunction = twoClassSummary)

knn2 = train(target ~ ., data = train_data1, method = 'knn', tuneLength = 80,
             trControl = trCrl2, preProcess = c('center', 'scale'), metric = 'ROC')
```


```{r}
# The larger the area under the ROC curve, the better the model performs. Therefore,
## we can see that when K is 97, the model performs the best.
knn2
```

```{r}
# The plot also shows that a K equals to 97 results in the greatest area under ROC.
plot(knn2)
```

```{r}
# The confusion matrix shows that the prediction accuracy is 80%. In this case, ROC
## does a worse job figuring out the model with the strongest predictive power than
## the accuracy.
confusionMatrix(predict(knn2, newdata = test_data1), as.factor(test_data1$target))
```

